The following table contains 16 models for music generation under different strategies.

| Publication Year | Title                                                                                        | Institution                         | Type of Model                              | Dataset                     | Paper                                                                                                 | Code |
|------------------|----------------------------------------------------------------------------------------------|-------------------------------------|--------------------------------------------|-----------------------------|-------------------------------------------------------------------------------------------------------|------|
| 1994             | GenJam                                                                                       | -                                   | Genetic Algorithm                           | -                           | -             
| 2002             | MusicScore                                                                                  | -                                   | Rule-Based Systems                          | -                           | -                                                   | -    |
| 2011             | Markov Melody Generator                                                                      | University of Massachusetts Lowell  | Markov Chain                                | -                           | -                                                   | -    |
| 2018             | Music Transformer                                                                            | Google Brain                                   | Transformer                                 | [JSB-Chorales](https://github.com/czhuang/JSB-Chorales-dataset)                           | [Link](https://arxiv.org/abs/1809.04281)                                                   | [Link](https://github.com/jason9693/MusicTransformer-pytorch)  |
| 2019             | MuseNet                                                                                      | OpenAI                              | Transformer                                 | ClassicalArchives, BitMidi, MAESTRO dataset  | [Link](https://doi.org/10.48550/arXiv.1907.04545)                                                    | [Link](https://github.com/openai/musenet)              |
| 2020             | Jukebox                                                                                      | -                                   | VQ-VAE                                      | 1.2 million songs           | [Link](https://doi.org/10.1109/TPAMI.2019.2905854)                                                    | [Link](https://github.com/openai/jukebox)              |
| 2020             | Foley Music                                                                                  | MIT                                   | Music from a silent video clip about people playing musical instruments.                             | -                           | [Link](https://arxiv.org/abs/2007.10984)                                                    | [Link](https://github.com/chuangg/Foley-Music)      |
| 2022             | Riffusion                                                                                    | Audio clip generation from text and images                                   | Diffusion Model                       | -                           | -                                                   | [Code is not available](https://github.com/riffusion/riffusion)             |
| 2023             | Mousai                                                                                       | ETH Zurich                                   | Diffusion Magnitude Autoencoder (DMAE)      | -                           | [Link](https://arxiv.org/pdf/2301.11757)                                                  | [Link](https://github.com/archinetai/audio-diffusion-pytorch)         |
| 2023             | MusicLM                                                                                      | Google Research                                   | Text to Music,  conditional music generation on text                 | [MusicCaps](https://www.kaggle.com/datasets/googleai/musiccaps)                           | [Link](https://arxiv.org/abs/2301.11325)                                                    | [Link](https://github.com/lucidrains/musiclm-pytorch)  |
| 2023             | Noise2Music                                                                                  | Google Research                                   | Diffusion Model                             | MuLaMCap                           | [Link](https://arxiv.org/abs/2302.03917)                                                    | -      |
| 2023             | CLAMP | Microsoft Research Asia                                   | Contrastive Learning                        | [WikiMT](https://huggingface.co/datasets/sander-wood/wikimusictext)                           | [Link](https://arxiv.org/abs/2304.11029)                                                             | [Link](https://github.com/microsoft/muzic/tree/main/clamp)    |
| 2023             | MeLoDy                                                            | ByteDance                           | LM-guided diffusion model, MeLoDy (M for music;L for LM; D for diffusion)                   | 257k hours of music         | [Link](https://Efficient-MeLoDy.github.io/)                                                           | -    |
| 2023             | Tango              | DeCLaRe Lab, Singapore University of Technology and Design, Singapore                                   | Latent Diffusion Model                      | [AudioCaps](https://audiocaps.github.io/)                           | [Link](https://arxiv.org/abs/2304.13731)                                                             | [Link](https://github.com/declare-lab/tango)    |
| 2023             | Tango 2 | DeCLaRe Lab, Singapore University of Technology and Design, Singapore                                   | Fine-tuned publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization)              | [Audio-Alpaca](https://huggingface.co/datasets/declare-lab/audio-alpaca)                            | [Link](https://arxiv.org/abs/2404.09956)                                                             | [Link](https://tango2-web.github.io/)    |
|2024 | MusicGen | Meta AI | Single Language Model that operates over several streams of compressed discrete music representation | | | [Link](https://github.com/facebookresearch/audiocraft) | 
