The following table contains 15 models for music generation under different strategies.

| Publication Year | Title                                                                                        | Institution                         | Type of Model                              | Dataset                     | Paper                                                                                                 | Code |
|------------------|----------------------------------------------------------------------------------------------|-------------------------------------|--------------------------------------------|-----------------------------|-------------------------------------------------------------------------------------------------------|------|
| 1994             | GenJam                                                                                       | Rochester Institute of Technology                                   | Uses selection, mutation, and crossover of musical sequences to generate new music                           | - | [Link](https://igm.rit.edu/~jabics/BilesICMC94.pdf)                           | -             
| 2010             | APOPCALEAPS                                                                                  | K.U.Leuven, Belgium                                   | Rule-Based Systems, Generates pop music based on chance rules                         | -                           | [Link](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=89f9ec84102de51636ad6df033acb59ac541f200)                                                   | -    |
| 2011             | Markov Melody Generator                                                                      | University of Massachusetts Lowell  | Markov Chain, Analyzes patterns in existing music to create new compositions                                | -                           | [Link](https://www.cs.uml.edu/ecg/uploads/AIfall11/SimoneHill.FinalPaper.MarkovMelodyGenerator.pdf)                                                   | -    |
| 2018             | Music Transformer                                                                            | Google Brain                                   | Transformer, Music generation with long-term structure                              | [JSB-Chorales](https://github.com/czhuang/JSB-Chorales-dataset)                           | [Link](https://arxiv.org/abs/1809.04281)                                                   | [Link](https://github.com/jason9693/MusicTransformer-pytorch)  |
| 2020             | Jukebox                                                                                      | OpenAI                                   | VQ-VAE                                      | 1.2 million songs           | [Link](https://doi.org/10.1109/TPAMI.2019.2905854)                                                    | [Link](https://github.com/openai/jukebox)              |
| 2020             | Foley Music                                                                                  | MIT                                   | Music from a silent video clip about people playing musical instruments.                             | -                           | [Link](https://arxiv.org/abs/2007.10984)                                                    | [Link](https://github.com/chuangg/Foley-Music)      |
| 2022             | Riffusion                                                                                    | -                                   | Diffusion Model, Audio clip generation from text and images                      | -                           | -                                                   | [Code is not available](https://github.com/riffusion/riffusion)             |
| 2023             | Mousai                                                                                       | ETH Zurich                                   | Diffusion Model, Generative music from textual prompts      | -                           | [Link](https://arxiv.org/pdf/2301.11757)                                                  | [Link](https://github.com/archinetai/audio-diffusion-pytorch)         |
| 2023             | MusicLM                                                                                      | Google Research                                   | Text to Music,  conditional music generation on text                 | [MusicCaps](https://www.kaggle.com/datasets/googleai/musiccaps)                           | [Link](https://arxiv.org/abs/2301.11325)                                                    | [Link](https://github.com/lucidrains/musiclm-pytorch)  |
| 2023             | Noise2Music                                                                                  | Google Research                                   | Diffusion Model, Generative music from textual prompts                             | MuLaMCap                           | [Link](https://arxiv.org/abs/2302.03917)                                                    | -      |
| 2023             | CLAMP | Microsoft Research Asia                                   | Contrastive Learning, Music Retrieval                        | [WikiMT](https://huggingface.co/datasets/sander-wood/wikimusictext)                           | [Link](https://arxiv.org/abs/2304.11029)                                                             | [Link](https://github.com/microsoft/muzic/tree/main/clamp)    |
| 2023             | MeLoDy                                                            | ByteDance                           | LM-guided diffusion model, MeLoDy (M for music;L for LM; D for diffusion)                   | 257k hours of music         | [Link](https://Efficient-MeLoDy.github.io/)                                                           | -    |
| 2023             | Tango              | DeCLaRe Lab, Singapore University of Technology and Design, Singapore                                   | Latent Diffusion Model                      | [AudioCaps](https://audiocaps.github.io/)                           | [Link](https://arxiv.org/abs/2304.13731)                                                             | [Link](https://github.com/declare-lab/tango)    |
| 2023             | Tango 2 | DeCLaRe Lab, Singapore University of Technology and Design, Singapore                                   | Fine-tuned publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization)              | [Audio-Alpaca](https://huggingface.co/datasets/declare-lab/audio-alpaca)                            | [Link](https://arxiv.org/abs/2404.09956)                                                             | [Link](https://tango2-web.github.io/)    |
|2024 | MusicGen | Meta AI | Single Language Model that operates over several streams of compressed discrete music representation | | | [Link](https://github.com/facebookresearch/audiocraft) | 


Additional repositories:
- Micorosft: https://github.com/microsoft/muzic
- Google: https://github.com/magenta https://magenta.withgoogle.com/
- Meta: https://github.com/facebookresearch/audiocraft
